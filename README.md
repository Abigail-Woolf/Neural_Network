# Neural_Network
Deep Learning Models

To optimize the deep neural network that I created, I tested a variety of hidden layer combinations with different activation functions. Although I tried a wide variety, I did not find any improvement in my Loss or Accuracy scores. There was once instance, where the Loss and Accuracy scores returned worse values than my original model. This happened when I used the Relu activation function on the output layer. I ended up chooseing four hidden layers with 3, 3, 6, 6 neurons respectively, although I did not see any change when altering the number of neurons or hidden layers. The Loss metricI acheived was ~0.69 and the Accuracy wasy ~0.53. In order to imporve this overall the overal learning, I would stick to the binary classification model, but I would manipulate the input data more. Perhaps there were outlier data points that threw off the training model. Maybe I dropped too many variables, or maybe I did not drop enough. I would experiement with mutiple combinations of variables to see if I could improve my machine learning metrics. 
